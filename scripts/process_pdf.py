"""
PDF'i GPT-4 Vision ile iÅŸleyip embeddings oluÅŸturan script
Bu script sadece bir kere Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r ve knowledge_base.json dosyasÄ± oluÅŸturur
"""

import os
import json
import base64
from typing import List, Dict
from pypdf import PdfReader
from pdf2image import convert_from_path
from openai import OpenAI
from dotenv import load_dotenv
import tiktoken

load_dotenv()

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def count_tokens(text: str) -> int:
    """Token sayÄ±sÄ±nÄ± hesapla"""
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    return len(encoding.encode(text))


def extract_text_from_pdf(pdf_path: str) -> str:
    """PDF'den text Ã§Ä±kar (gÃ¶rseller olmadan)"""
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        page_text = page.extract_text()
        if page_text:
            text += page_text + "\n"
    return text.strip()


def extract_images_and_analyze(pdf_path: str) -> List[Dict]:
    """
    PDF'i sayfa sayfa gÃ¶rsel olarak iÅŸle ve GPT-4 Vision ile analiz et
    Her sayfa iÃ§in gÃ¶rsel analizi + text extraction
    """
    print(f"ğŸ“„ PDF iÅŸleniyor: {pdf_path}")
    print("ğŸ–¼ï¸  Sayfalar gÃ¶rsele dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")

    # PDF'i gÃ¶rsellere Ã§evir
    images = convert_from_path(pdf_path, dpi=150)

    # PDF'den text de Ã§Ä±kar
    reader = PdfReader(pdf_path)

    pages_data = []

    for page_num, (image, page) in enumerate(zip(images, reader.pages), 1):
        print(f"âš™ï¸  Sayfa {page_num}/{len(images)} iÅŸleniyor...")

        # Sayfadan normal text Ã§Ä±kar
        text_content = page.extract_text() or ""

        # GÃ¶rseli base64'e Ã§evir
        import io
        buffer = io.BytesIO()
        image.save(buffer, format='PNG')
        image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')

        # GPT-4 Vision ile gÃ¶rseli analiz et
        try:
            vision_response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": """Bu gÃ¶rsel bir kullanÄ±cÄ± klavuzundan bir sayfa.
LÃ¼tfen bu sayfadaki:
1. TÃ¼m metinleri oku (Ã¶zellikle gÃ¶rsellerin iÃ§indeki yazÄ±lar)
2. GÃ¶rselleri/diyagramlarÄ±/ekran gÃ¶rÃ¼ntÃ¼lerini detaylÄ±ca aÃ§Ä±kla
3. AdÄ±m adÄ±m talimatlarÄ± varsa bunlarÄ± aÃ§Ä±kla
4. Butonlar, menÃ¼ler, arayÃ¼z elementlerini tanÄ±mla

TÃ¼rkÃ§e olarak, kullanÄ±cÄ±nÄ±n bu sayfayÄ± tam anlamasÄ± iÃ§in gereken tÃ¼m bilgiyi ver."""
                            },
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{image_base64}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=1000
            )

            vision_analysis = vision_response.choices[0].message.content

        except Exception as e:
            print(f"âš ï¸  GPT-4 Vision analizi baÅŸarÄ±sÄ±z (sayfa {page_num}): {e}")
            vision_analysis = ""

        # Text ve vision analizini birleÅŸtir
        combined_content = f"""
=== SAYFA {page_num} ===

[Metin Ä°Ã§eriÄŸi]
{text_content}

[GÃ¶rsel Analizi]
{vision_analysis}
"""

        pages_data.append({
            "page_number": page_num,
            "text_content": text_content,
            "vision_analysis": vision_analysis,
            "combined_content": combined_content.strip()
        })

        print(f"âœ“ Sayfa {page_num} tamamlandÄ±")

    return pages_data


def create_chunks(pages_data: List[Dict], chunk_size: int = 1500, overlap: int = 300) -> List[Dict]:
    """SayfalarÄ± anlamlÄ± parÃ§alara bÃ¶l"""
    chunks = []

    for page in pages_data:
        content = page["combined_content"]
        page_num = page["page_number"]

        # Basit overlapping chunks
        words = content.split()

        for i in range(0, len(words), chunk_size - overlap):
            chunk_words = words[i:i + chunk_size]
            chunk_text = " ".join(chunk_words)

            if len(chunk_text.strip()) > 100:  # Minimum chunk size
                chunks.append({
                    "text": chunk_text,
                    "page_number": page_num,
                    "chunk_index": len(chunks)
                })

    return chunks


def create_embeddings(chunks: List[Dict]) -> List[Dict]:
    """Her chunk iÃ§in embedding oluÅŸtur"""
    print(f"\nğŸ”® {len(chunks)} parÃ§a iÃ§in embeddings oluÅŸturuluyor...")

    enriched_chunks = []

    for i, chunk in enumerate(chunks, 1):
        print(f"  {i}/{len(chunks)}", end='\r')

        try:
            response = client.embeddings.create(
                model="text-embedding-3-small",
                input=chunk["text"]
            )

            embedding = response.data[0].embedding

            enriched_chunks.append({
                "text": chunk["text"],
                "page_number": chunk["page_number"],
                "chunk_index": chunk["chunk_index"],
                "embedding": embedding,
                "token_count": count_tokens(chunk["text"])
            })

        except Exception as e:
            print(f"\nâš ï¸  Embedding oluÅŸturulamadÄ± (chunk {i}): {e}")
            continue

    print(f"\nâœ“ {len(enriched_chunks)} embedding oluÅŸturuldu")
    return enriched_chunks


def save_knowledge_base(chunks: List[Dict], output_path: str):
    """Knowledge base'i JSON olarak kaydet"""
    knowledge_base = {
        "metadata": {
            "total_chunks": len(chunks),
            "total_tokens": sum(c["token_count"] for c in chunks),
            "embedding_model": "text-embedding-3-small",
            "vision_model": "gpt-4o"
        },
        "chunks": chunks
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(knowledge_base, f, ensure_ascii=False, indent=2)

    file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
    print(f"\nğŸ’¾ Knowledge base kaydedildi: {output_path}")
    print(f"   Dosya boyutu: {file_size:.2f} MB")


def main():
    print("=" * 60)
    print("  PDF Ä°ÅŸleme ve Knowledge Base OluÅŸturma")
    print("  GPT-4 Vision ile GÃ¶rsel Analizi Dahil")
    print("=" * 60)
    print()

    # PDF yolu
    pdf_path = "../data/Manuel utilisateur.docx.pdf"

    if not os.path.exists(pdf_path):
        print(f"âŒ PDF bulunamadÄ±: {pdf_path}")
        print("LÃ¼tfen PDF'i data/ klasÃ¶rÃ¼ne koyun.")
        return

    # 1. PDF'i iÅŸle (text + vision)
    print("\nğŸ“‹ AdÄ±m 1: PDF Analizi (GPT-4 Vision)")
    print("-" * 60)
    pages_data = extract_images_and_analyze(pdf_path)

    # 2. Chunks oluÅŸtur
    print("\nğŸ“‹ AdÄ±m 2: ParÃ§alara BÃ¶lme")
    print("-" * 60)
    chunks = create_chunks(pages_data)
    print(f"âœ“ {len(chunks)} parÃ§a oluÅŸturuldu")

    # 3. Embeddings oluÅŸtur
    print("\nğŸ“‹ AdÄ±m 3: Embeddings OluÅŸturma")
    print("-" * 60)
    enriched_chunks = create_embeddings(chunks)

    # 4. Kaydet
    print("\nğŸ“‹ AdÄ±m 4: Knowledge Base Kaydetme")
    print("-" * 60)
    output_path = "../data/knowledge_base.json"
    save_knowledge_base(enriched_chunks, output_path)

    print("\n" + "=" * 60)
    print("âœ… Ä°ÅŸlem TamamlandÄ±!")
    print("=" * 60)
    print("\nÃ–zet:")
    print(f"  â€¢ {len(pages_data)} sayfa iÅŸlendi")
    print(f"  â€¢ {len(enriched_chunks)} chunk oluÅŸturuldu")
    print(f"  â€¢ Knowledge base: {output_path}")
    print("\nArtÄ±k Vercel'e deploy edebilirsiniz!")


if __name__ == "__main__":
    main()
